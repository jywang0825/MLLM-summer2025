#!/usr/bin/env python3
"""
Check GPU-only execution for InternVL3 finetuning
Verifies that PyTorch is using CUDA and not falling back to CPU
"""

import torch
import os
import sys

def check_gpu_only():
    """Check if PyTorch is configured for GPU-only execution"""
    
    print("üîç Checking GPU-only execution configuration...")
    print("=" * 50)
    
    # Check CUDA availability
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"CUDA device count: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        print(f"Current CUDA device: {torch.cuda.current_device()}")
        print(f"CUDA device name: {torch.cuda.get_device_name()}")
        print(f"CUDA device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("‚ùå CUDA not available!")
        return False
    
    # Check environment variables
    print("\nüìã Environment Variables:")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
    print(f"PYTORCH_CUDA_ALLOC_CONF: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'Not set')}")
    print(f"DS_ACCELERATOR: {os.environ.get('DS_ACCELERATOR', 'Not set')}")
    
    # Test tensor operations on GPU
    print("\nüß™ Testing GPU tensor operations:")
    try:
        # Create tensors on GPU
        x = torch.randn(1000, 1000, device='cuda')
        y = torch.randn(1000, 1000, device='cuda')
        
        # Perform operations
        z = torch.matmul(x, y)
        
        print(f"‚úÖ Tensor device: {x.device}")
        print(f"‚úÖ Matrix multiplication successful on {z.device}")
        print(f"‚úÖ Result shape: {z.shape}")
        
        # Check memory usage
        print(f"‚úÖ GPU memory allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB")
        print(f"‚úÖ GPU memory cached: {torch.cuda.memory_reserved() / 1e6:.1f} MB")
        
    except Exception as e:
        print(f"‚ùå GPU tensor operation failed: {e}")
        return False
    
    # Test model loading on GPU
    print("\nü§ñ Testing model loading on GPU:")
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # Load a small model for testing
        model_name = "microsoft/DialoGPT-small"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
        
        # Move to GPU
        model = model.cuda()
        
        print(f"‚úÖ Model device: {next(model.parameters()).device}")
        print(f"‚úÖ Model loaded successfully on GPU")
        
        # Clean up
        del model, tokenizer
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"‚ùå Model loading test failed: {e}")
        return False
    
    print("\n‚úÖ All GPU-only checks passed!")
    print("üöÄ Ready for GPU-only InternVL3 finetuning")
    return True

if __name__ == "__main__":
    success = check_gpu_only()
    sys.exit(0 if success else 1) 